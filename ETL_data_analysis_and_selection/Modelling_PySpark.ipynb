{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMSXyYSKKNCpsxiNm19ee9c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Libraries\n","from pyspark.sql import functions as F\n","from pyspark.sql.window import Window\n","from pyspark.sql import Row\n","\n","# 1.1: Calculate proportions for the original dataset\n","def calculate_proportions_org(regitra_transeksta_final):\n","    \"\"\"\n","    Calculate proportions based on vehicle type and explanation.\n","\n","    :param regitra_transeksta_final: PySpark DataFrame with vehicle data.\n","    :return: DataFrame with total counts and proportions.\n","    \"\"\"\n","    total_count = regitra_transeksta_final.count()\n","    count_with_rida = regitra_transeksta_final.filter(F.col('rida_per_metus').isNotNull()).count()\n","\n","    # Group by vehicle type and explanation, then calculate total and valid counts that has milleage\n","    proportions_df = regitra_transeksta_final.groupBy('transporto_priemones_tipas', 'transporto_priemones_paaiskinimas') \\\n","        .agg(\n","            F.count('*').alias('total_count'),\n","            F.count(F.when(F.col('rida_per_metus').isNotNull(), 1)).alias('count_with_rida')\n","        )\n","\n","    # Calculate proportions as percentages of total and valid counts\n","    proportions_df = proportions_df.withColumn(\n","        'proportion_total',\n","        F.round((F.col('total_count') / total_count) * 100, 2)\n","    ).withColumn(\n","        'proportion_with_rida',\n","        F.round((F.col('count_with_rida') / count_with_rida) * 100, 2)\n","    )\n","\n","    return proportions_df\n","\n","# 1.2: Extract unique combinations of bus makes and models\n","def autobusu_markes_org(regitra_transeksta_final):\n","    \"\"\"\n","    Filter vehicle type 'K4' (buses) and retrieve unique combinations of 'cleaned_marke' and 'modelis',\n","    along with record counts and percentages.\n","\n","    :param regitra_transeksta_final: PySpark DataFrame with vehicle data.\n","    :return: Sorted DataFrame with unique values of 'cleaned_marke' and 'modelis', including counts and percentages.\n","    \"\"\"\n","\n","    filtered_df = regitra_transeksta_final.filter(F.col('transporto_priemones_tipas') == 'K4')\n","\n","    # Group by 'cleaned_marke' and 'cleaned_modelis' and count occurrences\n","    grouped_df = filtered_df.groupBy('cleaned_marke', 'cleaned_modelis') \\\n","                            .agg(F.count('*').alias('count'))\n","\n","    total_count = filtered_df.count()\n","\n","    # Add a percentage column to represent the proportion of each combination\n","    result_df = grouped_df.withColumn('percentage', F.round((F.col('count') / total_count) * 100, 2))\n","\n","    sorted_df = result_df.orderBy(F.col('count').desc())\n","\n","    return sorted_df\n","\n","\n","\n","# 1.3: Calculate proportions based on fuel type\n","def calculate_proportions_with_fuel_org(regitra_transeksta_final):\n","    \"\"\"\n","    Calculate proportions considering fuel types by combining 'degalai' and 'papildomi_degalai_1'.\n","\n","    :param regitra_transeksta_final: PySpark DataFrame with vehicle data.\n","    :return: DataFrame with proportions grouped by vehicle type, explanation, and fuel combination.\n","    \"\"\"\n","\n","    # Filter out rows where 'transporto_priemones_tipas' is NULL\n","    regitra_transeksta_final = regitra_transeksta_final.filter(F.col('transporto_priemones_tipas').isNotNull())\n","\n","    # Create a new column combining 'degalai' and 'papildomi_degalai_1' if both exist\n","    regitra_transeksta_final = regitra_transeksta_final.withColumn(\n","        'fuel_combined',\n","        F.when(F.col('papildomi_degalai_1').isNotNull(), F.concat(F.col('degalai'), F.lit(', '), F.col('papildomi_degalai_1')))\n","        .otherwise(F.col('degalai'))\n","    )\n","\n","    # Get total record count and count of non-NULL 'rida_per_metus' (milleage per year)\n","    total_count = regitra_transeksta_final.count()\n","    count_with_rida = regitra_transeksta_final.filter(F.col('rida_per_metus').isNotNull()).count()\n","\n","    # Group by vehicle type, explanation, and fuel combination; calculate total and valid counts\n","    proportions_df = regitra_transeksta_final.groupBy('transporto_priemones_tipas', 'transporto_priemones_paaiskinimas', 'fuel_combined') \\\n","        .agg(\n","            F.count('*').alias('total_count'),\n","            F.count(F.when(F.col('rida_per_metus').isNotNull(), 1)).alias('count_with_rida')\n","        )\n","\n","    # Add columns for proportions as percentages\n","    proportions_df = proportions_df.withColumn(\n","        'proportion_total',\n","        F.round((F.col('total_count') / total_count) * 100, 4)\n","    ).withColumn(\n","        'proportion_with_rida',\n","        F.round((F.col('count_with_rida') / count_with_rida) * 100, 4)\n","    )\n","\n","    # Add a summary row for each fuel type\n","    total_rows = regitra_transeksta_final.groupBy('fuel_combined') \\\n","        .agg(\n","            F.count('*').alias('total_count'),\n","            F.count(F.when(F.col('rida_per_metus').isNotNull(), 1)).alias('count_with_rida')\n","        ) \\\n","        .withColumn('transporto_priemones_tipas', F.lit('K0')) \\\n","        .withColumn('transporto_priemones_paaiskinimas', F.lit('Iš viso'))\n","\n","    # Add proportion columns to the summary rows\n","    total_rows = total_rows.withColumn(\n","        'proportion_total',\n","        F.round((F.col('total_count') / total_count) * 100, 4)\n","    ).withColumn(\n","        'proportion_with_rida',\n","        F.round((F.col('count_with_rida') / count_with_rida) * 100, 4)\n","    )\n","\n","    # Combine the detailed proportions with the summary rows\n","    proportions_df = proportions_df.unionByName(total_rows)\n","\n","    # Sort by vehicle type and fuel combination\n","    proportions_df = proportions_df.orderBy('transporto_priemones_tipas', 'fuel_combined')\n","\n","    return proportions_df\n","\n","\n","# 2.0: Outlier detection using IQR 1.5 method\n","def outlier_detection(regitra_transeksta_final):\n","    \"\"\"\n","    Detects outliers in 'rida_per_metus' column using the IQR method.\n","\n","    :param regitra_transeksta_final: PySpark DataFrame with vehicle data and milleage information.\n","    :return: DataFrame with an additional column 'outlier_rida_per_tipa' marking outliers (1 for outlier, 0 otherwise).\n","    \"\"\"\n","    # Filter rows where 'rida_per_metus' is not NULL\n","    filtered_df = regitra_transeksta_final.filter(F.col('rida_per_metus').isNotNull())\n","\n","    # Define a window specification partitioned by 'tp_pavadinimas'\n","    window_spec = Window.partitionBy('tp_pavadinimas')\n","\n","    # Calculate Q1 (25th percentile)\n","    q1_df = filtered_df.withColumn(\n","        'Q1',\n","        F.expr('percentile_approx(rida_per_metus, 0.25)').over(window_spec)\n","    )\n","\n","    # Calculate Q3 (75th percentile)\n","    q3_df = q1_df.withColumn(\n","        'Q3',\n","        F.expr('percentile_approx(rida_per_metus, 0.75)').over(window_spec)\n","    )\n","\n","    # Calculate the IQR (Q3 - Q1)\n","    iqr_df = q3_df.withColumn(\n","        'IQR',\n","        F.col('Q3') - F.col('Q1')\n","    )\n","\n","    # Calculate lower and upper bounds for outlier detection\n","    iqr_boundaries_df = iqr_df.withColumn(\n","        'lower_bound',\n","        F.col('Q1') - 1.5 * F.col('IQR')\n","    ).withColumn(\n","        'upper_bound',\n","        F.col('Q3') + 1.5 * F.col('IQR')\n","    )\n","\n","    # Mark rows as outliers (1) or not (0) in the 'outlier_rida_per_tipa' column\n","    outlier_df = iqr_boundaries_df.withColumn(\n","        'outlier_rida_per_tipa',\n","        F.when(\n","            (F.col('rida_per_metus') < F.col('lower_bound')) |\n","            (F.col('rida_per_metus') > F.col('upper_bound')), 1\n","        ).otherwise(0)\n","    )\n","\n","    return outlier_df\n","\n","\n","# 2.1 Count outliers grouped by vehicle type\n","def outliers_results(outlier_detection):\n","    \"\"\"\n","    Groups data by 'tp_pavadinimas' and 'outlier_rida_per_tipa' and counts occurrences.\n","\n","    :param outlier_detection: DataFrame with outlier detection results.\n","    :return: DataFrame with grouped counts of outliers and non-outliers.\n","    \"\"\"\n","    # Group by vehicle type and outlier flag, then count occurrences\n","    outliers_count_df = outlier_detection.groupBy('tp_pavadinimas', 'outlier_rida_per_tipa') \\\n","        .agg(F.count('outlier_rida_per_tipa').alias('count')) \\\n","        .orderBy('tp_pavadinimas', 'outlier_rida_per_tipa')\n","\n","    return outliers_count_df\n","\n","\n","\n","# 3.0 Set outliers to NULL\n","def set_outliers_to_null(outlier_detection):\n","    \"\"\"\n","    Sets 'rida_per_diena' and 'rida_per_metus' values to NULL for rows marked as outliers.\n","\n","    :param outlier_detection: DataFrame with outlier detection results.\n","    :return: Updated DataFrame with outlier values set to NULL.\n","    \"\"\"\n","    # Replace values with NULL for rows where 'outlier_rida_per_tipa' equals 1\n","    updated_df = outlier_detection.withColumn(\n","        'rida_per_diena',\n","        F.when(F.col('outlier_rida_per_tipa') == 1, None).otherwise(F.col('rida_per_diena'))\n","    ).withColumn(\n","        'rida_per_metus',\n","        F.when(F.col('outlier_rida_per_tipa') == 1, None).otherwise(F.col('rida_per_metus'))\n","    )\n","\n","    return updated_df\n","\n","\n","# 3.1 Calculate proportions after removing outliers\n","def calculate_proportions(set_outliers_to_null):\n","    \"\"\"\n","    Calculates proportions based on vehicle type and explanation after removing outliers.\n","\n","    :param set_outliers_to_null: DataFrame with outlier values set to NULL.\n","    :return: DataFrame with calculated proportions.\n","    \"\"\"\n","    # Calculate the total number of records\n","    total_count = set_outliers_to_null.count()\n","\n","    # Count records where 'rida_per_metus' is not NULL\n","    count_with_rida = set_outliers_to_null.filter(F.col('rida_per_metus').isNotNull()).count()\n","\n","    # Group by vehicle type and explanation, then calculate proportions\n","    proportions_df = set_outliers_to_null.groupBy('transporto_priemones_tipas', 'transporto_priemones_paaiskinimas') \\\n","        .agg(\n","            F.count('*').alias('total_count'),\n","            F.count(F.when(F.col('rida_per_metus').isNotNull(), 1)).alias('count_with_rida')\n","        )\n","\n","    # Calculate proportions as percentages\n","    proportions_df = proportions_df.withColumn(\n","        'proportion_total',\n","        F.round((F.col('total_count') / total_count) * 100, 2)\n","    ).withColumn(\n","        'proportion_with_rida',\n","        F.round((F.col('count_with_rida') / count_with_rida) * 100, 2)\n","    )\n","\n","    return proportions_df\n","\n","\n","# 3.2 Extract bus makes and models with mileage\n","def autobusu_markes_su_rida(set_outliers_to_null):\n","    \"\"\"\n","    Filters vehicle type 'K4' (buses) and retrieves unique combinations of 'cleaned_marke', 'modelis', and 'degalai',\n","    including record counts and percentages.\n","\n","    :param set_outliers_to_null: DataFrame with outlier values set to NULL.\n","    :return: Sorted DataFrame with unique bus make-model-fuel combinations and percentages.\n","    \"\"\"\n","    # Filter for vehicle type 'K4' (buses)\n","    filtered_df = set_outliers_to_null.filter(F.col('transporto_priemones_tipas') == 'K4')\n","\n","    # Group by 'cleaned_marke', 'cleaned_modelis', and 'degalai', then count occurrences\n","    grouped_df = filtered_df.groupBy('cleaned_marke', 'cleaned_modelis', 'degalai') \\\n","                            .agg(F.count('*').alias('count'))\n","\n","    # Calculate the total number of records in the filtered dataset\n","    total_count = filtered_df.count()\n","\n","    # Add a percentage column to represent the proportion of each combination\n","    result_df = grouped_df.withColumn('percentage', F.round((F.col('count') / total_count) * 100, 2))\n","\n","    # Sort by count in descending order\n","    sorted_df = result_df.orderBy(F.col('count').desc())\n","\n","    return sorted_df\n","\n","# 3.3  Trolleybuses unique models and makers with fuel type and record counts and percentages\n","def troleibusu_markes_su_rida(set_outliers_to_null):\n","    \"\"\"\n","    Retrieve unique combinations of 'cleaned_marke', 'modelis', and 'degalai' for vehicle type 'K5'\n","    (trolleybuses), including record counts and percentages. Sort results by count in descending order.\n","\n","    :param set_outliers_to_null: PySpark DataFrame with processed vehicle data.\n","    :return: Sorted DataFrame with unique combinations of 'cleaned_marke', 'modelis', and 'degalai',\n","             including record counts and percentages.\n","    \"\"\"\n","    # Filter the dataset for vehicle type 'K5' (trolleybuses)\n","    filtered_df = set_outliers_to_null.filter(F.col('transporto_priemones_tipas') == 'K5')\n","\n","    # Group by 'cleaned_marke', 'cleaned_modelis', and 'degalai', calculating counts\n","    grouped_df = filtered_df.groupBy('cleaned_marke', 'cleaned_modelis', 'degalai') \\\n","                            .agg(F.count('*').alias('count'))\n","\n","    # Calculate the total number of records in the filtered dataset\n","    total_count = filtered_df.count()\n","\n","    # Add a column for percentage of each unique combination\n","    result_df = grouped_df.withColumn('percentage', F.round((F.col('count') / total_count) * 100, 2))\n","\n","    # Sort by count in descending order\n","    sorted_df = result_df.orderBy(F.col('count').desc())\n","\n","    return sorted_df\n","\n","#3.4 Proportions for records based on vehicle type, fuel type\n","\n","def calculate_proportions_with_fuel(set_outliers_to_null):\n","    \"\"\"\n","    Calculate proportions for records based on vehicle type, explanation, and fuel type combination\n","    ('degalai' and 'papildomi_degalai_1'). Includes handling of missing values.\n","\n","    :param set_outliers_to_null: PySpark DataFrame with processed vehicle data.\n","    :return: DataFrame with proportions grouped by vehicle type, explanation, and combined fuel types.\n","    \"\"\"\n","    # Remove records where 'transporto_priemones_tipas' is NULL\n","    set_outliers_to_null = set_outliers_to_null.filter(F.col('transporto_priemones_tipas').isNotNull())\n","\n","    # Create a new column combining 'degalai' and 'papildomi_degalai_1' if both exist\n","    set_outliers_to_null = set_outliers_to_null.withColumn(\n","        'fuel_combined',\n","        F.when(F.col('papildomi_degalai_1').isNotNull(), F.concat(F.col('degalai'), F.lit(', '), F.col('papildomi_degalai_1')))\n","        .otherwise(F.col('degalai'))\n","    )\n","\n","    # Calculate total record count and count of non-NULL 'rida_per_metus'\n","    total_count = set_outliers_to_null.count()\n","    count_with_rida = set_outliers_to_null.filter(F.col('rida_per_metus').isNotNull()).count()\n","\n","    # Group by vehicle type, explanation, and fuel combination; calculate counts\n","    proportions_df = set_outliers_to_null.groupBy('transporto_priemones_tipas', 'transporto_priemones_paaiskinimas', 'fuel_combined') \\\n","        .agg(\n","            F.count('*').alias('total_count'),\n","            F.count(F.when(F.col('rida_per_metus').isNotNull(), 1)).alias('count_with_rida')\n","        )\n","\n","    # Add proportions as percentages\n","    proportions_df = proportions_df.withColumn(\n","        'proportion_total',\n","        F.round((F.col('total_count') / total_count) * 100, 4)\n","    ).withColumn(\n","        'proportion_with_rida',\n","        F.round((F.col('count_with_rida') / count_with_rida) * 100, 4)\n","    )\n","\n","    # Add summary row ('K0') for each fuel type\n","    total_rows = set_outliers_to_null.groupBy('fuel_combined') \\\n","        .agg(\n","            F.count('*').alias('total_count'),\n","            F.count(F.when(F.col('rida_per_metus').isNotNull(), 1)).alias('count_with_rida')\n","        ) \\\n","        .withColumn('transporto_priemones_tipas', F.lit('K0')) \\\n","        .withColumn('transporto_priemones_paaiskinimas', F.lit('Total'))\n","\n","    # Add proportions to the summary rows\n","    total_rows = total_rows.withColumn(\n","        'proportion_total',\n","        F.round((F.col('total_count') / total_count) * 100, 4)\n","    ).withColumn(\n","        'proportion_with_rida',\n","        F.round((F.col('count_with_rida') / count_with_rida) * 100, 4)\n","    )\n","\n","    # Combine detailed proportions with summary rows\n","    proportions_df = proportions_df.unionByName(total_rows)\n","\n","    # Sort by vehicle type and combined fuel type\n","    proportions_df = proportions_df.orderBy('transporto_priemones_tipas', 'fuel_combined')\n","\n","    return proportions_df\n","\n","\n","# 3.5 Check for missing values\n","def check_na_values(set_outliers_to_null):\n","    \"\"\"\n","    Check for missing (NULL) values in selected columns. Also calculates the percentage of missing\n","    values in each column.\n","\n","    :param set_outliers_to_null: PySpark DataFrame with processed vehicle data after handling outliers.\n","    :return: DataFrame with NA counts and percentages for each selected column.\n","    \"\"\"\n","    # Columns to check for NA values\n","    columns_to_check = ['cleaned_marke', 'cleaned_modelis', 'transporto_priemones_tipas', 'variklio_turis', 'galia', 'degalai', 'nuosava_mase', 'rida_per_metus']\n","\n","    # Ensure the columns exist in the dataset\n","    existing_columns = [col for col in columns_to_check if col in set_outliers_to_null.columns]\n","\n","    # Total record count\n","    total_count = set_outliers_to_null.count()\n","\n","    # Generate NA count and percentage for each column\n","    na_count_expressions = []\n","    for col in existing_columns:\n","        na_count_expressions.append(F.count(F.when(F.col(col).isNull(), col)).alias(f'{col}_na_count'))\n","        na_count_expressions.append(\n","            (F.round((F.count(F.when(F.col(col).isNull(), col)) / total_count) * 100, 2)).alias(f'{col}_na_percentage')\n","        )\n","\n","    # Create a DataFrame with the NA results\n","    na_count_df = set_outliers_to_null.select(*na_count_expressions)\n","\n","    return na_count_df\n","\n","# 3.6 Remove rows with missing data in specified columns\n","def delete_na_values(set_outliers_to_null):\n","    \"\"\"\n","    Remove rows with missing (NULL) values in specified columns.\n","\n","    :param set_outliers_to_null: PySpark DataFrame with processed vehicle data after handling outliers.\n","    :return: Cleaned DataFrame with rows containing NULL values in specified columns removed.\n","    \"\"\"\n","\n","    columns_to_check = ['cleaned_marke', 'cleaned_modelis', 'transporto_priemones_tipas', 'variklio_turis', 'galia', 'degalai', 'nuosava_mase', 'rida_per_metus', 'pag_metai']\n","\n","    # Drop rows with NULL values in the specified columns\n","    df_cleaned = set_outliers_to_null.dropna(subset=columns_to_check)\n","\n","    return df_cleaned\n","\n","# 3.7 Correlation analysis of milleage and vehicle owner age\n","def calculate_correlation_amzius_rida(set_outliers_to_null):\n","    \"\"\"\n","    Calculate correlation between 'rida_per_metus' and 'amzius'. Filters out NA values before calculation.\n","\n","    :param set_outliers_to_null: PySpark DataFrame with processed vehicle data after handling outliers.\n","    :return: Spark DataFrame with correlation result.\n","    \"\"\"\n","    # Filter rows with non-NULL 'rida_per_metus' and 'amzius'\n","    filtered_df = set_outliers_to_null.filter(F.col('rida_per_metus').isNotNull() & F.col('amzius').isNotNull())\n","\n","    # Calculate correlation\n","    correlation_value = filtered_df.stat.corr('rida_per_metus', 'amzius')\n","\n","    # Return correlation as a Spark DataFrame\n","    result_df = set_outliers_to_null.sparkSession.createDataFrame([Row(correlation_rida_amzius=correlation_value)])\n","\n","    return result_df\n","\n","\n","# 3.8 Correlation analysis of mileage, age for each municipality\n","def calculate_correlation_with_savivaldybe(set_outliers_to_null):\n","    \"\"\"\n","    Calculates the correlation between 'rida_per_metus' (mileage per year) and 'amzius' (age)\n","    separately for each municipality ('savivaldybe'). It removes NA values before calculation.\n","\n","    :param set_outliers_to_null: PySpark DataFrame with vehicle data.\n","    :return: DataFrame with correlation results by municipality.\n","    \"\"\"\n","    # Columns to check for correlation\n","    corr_columns = ['rida_per_metus', 'amzius']\n","\n","    # Municipality column\n","    savivaldybe_col = 'savivaldybe'\n","\n","    # Filter out rows with null values in the columns used for correlation\n","    filtered_df = set_outliers_to_null.filter(F.col(corr_columns[0]).isNotNull() & F.col(corr_columns[1]).isNotNull())\n","\n","    # Get all unique municipalities\n","    unique_savivaldybes = filtered_df.select(savivaldybe_col).distinct().collect()\n","\n","    # Create an empty list to store correlation results\n","    correlation_results = []\n","\n","    # Loop through each municipality and calculate the correlation\n","    for savivaldybe in unique_savivaldybes:\n","        savivaldybe_name = savivaldybe[savivaldybe_col]\n","\n","        # Filter data for the specific municipality\n","        filtered_savivaldybe_df = filtered_df.filter(F.col(savivaldybe_col) == savivaldybe_name)\n","\n","        # Calculate correlation between 'rida_per_metus' and 'amzius'\n","        correlation_value = filtered_savivaldybe_df.stat.corr(corr_columns[0], corr_columns[1])\n","\n","        # Append the results to the list\n","        correlation_results.append((savivaldybe_name, correlation_value))\n","\n","    # Convert the results to a DataFrame\n","    results_df = set_outliers_to_null.sparkSession.createDataFrame(correlation_results, [savivaldybe_col, 'correlation_rida_per_metus_amzius'])\n","\n","    return results_df\n","\n","#3.6.1\n","def categorize_savivaldybe(delete_na_values):\n","    \"\"\"\n","    Categorizes 'savivaldybe' (municipality) into three categories: 'Rajono' (rural),\n","    'Miesto' (city), and 'Didmiesčio' (metropolitan).\n","\n","    :param delete_na_values: PySpark DataFrame.\n","    :return: DataFrame with an additional column 'savivaldybe_category'.\n","    \"\"\"\n","\n","    # Define a mapping function for categorization\n","    def savivaldybe_category(savivaldybe):\n","        if 'R. SAV.' in savivaldybe:\n","            return 'Rajono'\n","        elif savivaldybe in ['VILNIAUS M. SAV.', 'KAUNO M. SAV.']:\n","            return 'Didmiesčio'\n","        else:\n","            return 'Miesto'\n","\n","    # Register the function as a UDF\n","    categorize_udf = F.udf(savivaldybe_category, T.StringType())\n","\n","    # Apply the UDF to the dataframe\n","    categorized_df = delete_na_values.withColumn('savivaldybe_category', categorize_udf(F.col('savivaldybe')))\n","\n","    return categorized_df\n","\n","\n","\n","# 3.6.1.1 Correlation analysis by municipality category\n","def calculate_correlation_with_savivaldybe_category(categorize_savivaldybe):\n","    \"\"\"\n","    Calculates the correlation between 'rida_per_metus' and 'amzius' (age),\n","    separately for each municipality category.\n","\n","    :param categorize_savivaldybe: PySpark DataFrame with categorized municipality data.\n","    :return: DataFrame with correlation results by municipality category.\n","    \"\"\"\n","\n","    # Columns to check for correlation\n","    corr_columns = ['rida_per_metus', 'amzius']\n","\n","    # Municipality category column\n","    savivaldybe_col = 'savivaldybe_category'\n","\n","    # Filter out rows with null values in the columns used for correlation\n","    filtered_df = categorize_savivaldybe.filter(F.col(corr_columns[0]).isNotNull() & F.col(corr_columns[1]).isNotNull())\n","\n","    # Get all unique municipality categories\n","    unique_savivaldybes = filtered_df.select(savivaldybe_col).distinct().collect()\n","\n","    # Create an empty list to store correlation results\n","    correlation_results = []\n","\n","    # Loop through each category and calculate the correlation\n","    for savivaldybe in unique_savivaldybes:\n","        savivaldybe_name = savivaldybe[savivaldybe_col]\n","\n","        # Filter data for the specific category\n","        filtered_savivaldybe_df = filtered_df.filter(F.col(savivaldybe_col) == savivaldybe_name)\n","\n","        # Calculate correlation between 'rida_per_metus' and 'amzius'\n","        correlation_value = filtered_savivaldybe_df.stat.corr(corr_columns[0], corr_columns[1])\n","\n","        # Append the results to the list\n","        correlation_results.append((savivaldybe_name, correlation_value))\n","\n","    # Convert the results to a DataFrame\n","    results_df = categorize_savivaldybe.sparkSession.createDataFrame(correlation_results, [savivaldybe_col, 'correlation_rida_per_metus_amzius'])\n","\n","    return results_df\n","\n","# 3.6.1.2 Categorize age\n","def categorize_amzius(categorize_savivaldybe):\n","    \"\"\"\n","    Function to categorize the 'amzius' column into different age groups,\n","    create a numerical category for each group, and add explanations for each category.\n","\n","    :param categorize_savivaldybe: Input PySpark DataFrame with an 'amzius' column.\n","    :return: Updated PySpark DataFrame with 'amzius_categories', 'amzius_numerical',\n","             and 'amzius_categories_paaiskinimas' columns.\n","    \"\"\"\n","\n","    # Define the conditions for age categories\n","    categorize_savivaldybe = categorize_savivaldybe.withColumn(\n","        'amzius_categories',\n","        F.when((F.col('amzius') >= 18) & (F.col('amzius') < 25), '18 to 25')\n","         .when((F.col('amzius') >= 25) & (F.col('amzius') < 30), '25 to 30')\n","         .when((F.col('amzius') >= 30) & (F.col('amzius') < 40), '30 to 40')\n","         .when((F.col('amzius') >= 40) & (F.col('amzius') < 50), '40 to 50')\n","         .when((F.col('amzius') >= 50) & (F.col('amzius') < 60), '50 to 60')\n","         .when((F.col('amzius') >= 60) & (F.col('amzius') < 70), '60 to 70')\n","         .otherwise('more than 70')\n","    )\n","\n","    # Map the categories to numerical values\n","    categorize_savivaldybe = categorize_savivaldybe.withColumn(\n","        'amzius_numerical',\n","        F.when(F.col('amzius_categories') == '18 to 25', 1)\n","         .when(F.col('amzius_categories') == '25 to 30', 2)\n","         .when(F.col('amzius_categories') == '30 to 40', 3)\n","         .when(F.col('amzius_categories') == '40 to 50', 4)\n","         .when(F.col('amzius_categories') == '50 to 60', 5)\n","         .when(F.col('amzius_categories') == '60 to 70', 6)\n","         .otherwise(7)\n","    )\n","\n","    # Add the explanation (_paaiskinimas) for each category\n","    categorize_savivaldybe = categorize_savivaldybe.withColumn(\n","        'amzius_categories_paaiskinimas',\n","        F.when(F.col('amzius_categories') == '18 to 25', 'Young Adults')\n","         .when(F.col('amzius_categories') == '25 to 30', 'Adults starting career')\n","         .when(F.col('amzius_categories') == '30 to 40', 'Established Adults')\n","         .when(F.col('amzius_categories') == '40 to 50', 'Middle-aged Adults')\n","         .when(F.col('amzius_categories') == '50 to 60', 'Pre-retirement Adults')\n","         .when(F.col('amzius_categories') == '60 to 70', 'Early Retirement')\n","         .otherwise('Seniors')\n","    )\n","\n","    return categorize_savivaldybe\n","\n","# 3.6.1.3 Correlation analysis of mileage and age\n","def calculate_correlation_with_savivaldybe(categorize_amzius):\n","    \"\"\"\n","    Calculate correlation between 'rida_per_metus' and 'amzius' for each municipality.\n","\n","    :param categorize_amzius: PySpark DataFrame containing 'rida_per_metus', 'amzius', and 'savivaldybe'.\n","    :return: DataFrame with correlations for each municipality.\n","    \"\"\"\n","    # Columns to analyze correlation\n","    corr_columns = ['rida_per_metus', 'amzius']\n","    # Municipality column\n","    savivaldybe_col = 'savivaldybe'\n","\n","    # Filter rows where 'rida_per_metus' or 'amzius' is NULL\n","    filtered_df = categorize_amzius.filter(\n","        F.col(corr_columns[0]).isNotNull() & F.col(corr_columns[1]).isNotNull()\n","    )\n","\n","    # Get all unique municipalities\n","    unique_savivaldybes = filtered_df.select(savivaldybe_col).distinct().collect()\n","\n","    # List to store correlation results\n","    correlation_results = []\n","\n","    # Iterate over each municipality and calculate correlation\n","    for savivaldybe in unique_savivaldybes:\n","        savivaldybe_name = savivaldybe[savivaldybe_col]\n","        # Filter for the current municipality\n","        filtered_savivaldybe_df = filtered_df.filter(F.col(savivaldybe_col) == savivaldybe_name)\n","        # Calculate correlation\n","        correlation_value = filtered_savivaldybe_df.stat.corr(corr_columns[0], corr_columns[1])\n","        # Append the results\n","        correlation_results.append((savivaldybe_name, correlation_value))\n","\n","    # Convert results to a DataFrame\n","    results_df = categorize_amzius.sparkSession.createDataFrame(\n","        correlation_results, [savivaldybe_col, 'correlation_rida_per_metus_amzius']\n","    )\n","\n","    return results_df\n","\n","\n","\n","# 3.6.1.4 Categorizing ages into groups\n","def categorize_age(categorize_savivaldybe):\n","    \"\"\"\n","    Categorize 'amzius' into age groups and add numerical categories.\n","\n","    :param categorize_savivaldybe: PySpark DataFrame containing 'amzius'.\n","    :return: Updated DataFrame with 'amzius_categories' and 'amzius_numerical'.\n","    \"\"\"\n","    # Define age group conditions\n","    categorize_savivaldybe = categorize_savivaldybe.withColumn(\n","        'amzius_categories',\n","        F.when((F.col('amzius') >= 18) & (F.col('amzius') < 25), '18 to 25')\n","        .when((F.col('amzius') >= 25) & (F.col('amzius') < 30), '25 to 30')\n","        .when((F.col('amzius') >= 30) & (F.col('amzius') < 40), '30 to 40')\n","        .when((F.col('amzius') >= 40) & (F.col('amzius') < 50), '40 to 50')\n","        .when((F.col('amzius') >= 50) & (F.col('amzius') < 60), '50 to 60')\n","        .when((F.col('amzius') >= 60) & (F.col('amzius') < 70), '60 to 70')\n","        .otherwise('70 and above')\n","    )\n","\n","    # Assign numerical values to categories\n","    categorize_savivaldybe = categorize_savivaldybe.withColumn(\n","        'amzius_numerical',\n","        F.when(F.col('amzius_categories') == '18 to 25', 1)\n","        .when(F.col('amzius_categories') == '25 to 30', 2)\n","        .when(F.col('amzius_categories') == '30 to 40', 3)\n","        .when(F.col('amzius_categories') == '40 to 50', 4)\n","        .when(F.col('amzius_categories') == '50 to 60', 5)\n","        .when(F.col('amzius_categories') == '60 to 70', 6)\n","        .otherwise(7)\n","    )\n","\n","    return categorize_savivaldybe\n","\n","\n","# Clustering method:\n","# 4.1. K means clustering method for grouped millage and municipality category to get k and silhouette scores:\n","from pyspark.ml.clustering import KMeans\n","from pyspark.ml.feature import VectorAssembler, StandardScaler\n","from pyspark.ml.evaluation import ClusteringEvaluator\n","from pyspark.sql import functions as F\n","from pyspark.ml import Pipeline\n","from pyspark.sql.window import Window\n","from pyspark.sql import SparkSession\n","\n","def train_kmeans_on_grouped_ridas(sample_dataset_final, k_min=2, k_max=10):\n","    \"\"\"\n","    Trains optimized K-means clustering on grouped mileage data by vehicle type and municipality category,\n","    calculating average mileage for each cluster and average fuel consumption.\n","\n","    :param sample_dataset_final: PySpark DataFrame with columns 'rida_per_metus', 'transporto_priemones_tipas',\n","                                  'savivaldybe_category', and others.\n","    :param k_min: Minimum number of clusters (default 2).\n","    :param k_max: Maximum number of clusters (default 10).\n","    :return: PySpark DataFrame with Silhouette scores for each k, average mileage, fuel consumption, and grouping information by clusters.\n","    \"\"\"\n","\n","    # Check if required columns are present\n","    required_columns = ['rida_per_metus', 'amzius', 'transporto_priemones_tipas', 'savivaldybe_category']\n","    for col in required_columns:\n","        if col not in sample_dataset_final.columns:\n","            raise ValueError(f\"Column '{col}' is missing from the dataset.\")\n","\n","    # Filter rows with missing values in required columns\n","    sample_dataset_final = sample_dataset_final.filter(\n","        (F.col('rida_per_metus').isNotNull()) &\n","        (F.col('amzius').isNotNull()) &\n","        (F.col('transporto_priemones_tipas').isNotNull()) &\n","        (F.col('savivaldybe_category').isNotNull())\n","    )\n","\n","    # Group data by vehicle type and municipality category, calculate mean values for clustering features\n","    grouped_data = sample_dataset_final.groupBy('transporto_priemones_tipas', 'savivaldybe_category')\\\n","        .agg(F.mean('rida_per_metus').alias('avg_rida_per_metus'),\n","             F.mean('amzius').alias('avg_amzius'))\n","\n","    # Create a feature vector from the grouped average values\n","    assembler = VectorAssembler(inputCols=['avg_rida_per_metus', 'avg_amzius'], outputCol='features', handleInvalid='skip')\n","    dataset = assembler.transform(grouped_data)\n","\n","    # Normalize features\n","    scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n","\n","    # Optimize K-means clustering\n","    silhouette_scores = []\n","    best_silhouette = float('-inf')\n","    best_k = None\n","    best_model = None\n","    best_train_predictions = None\n","\n","    for k in range(k_min, k_max + 1):\n","        kmeans = KMeans(k=k, seed=1, featuresCol='scaled_features', predictionCol='cluster')\n","        pipeline = Pipeline(stages=[scaler, kmeans])\n","        model = pipeline.fit(dataset)\n","\n","        # Apply the model to the data\n","        predictions = model.transform(dataset)\n","\n","        # Calculate Silhouette score\n","        evaluator = ClusteringEvaluator(predictionCol='cluster', featuresCol='scaled_features')\n","        silhouette_train = evaluator.evaluate(predictions)\n","        silhouette_scores.append((k, silhouette_train))\n","\n","        # Select the best k\n","        if silhouette_train > best_silhouette:\n","            best_silhouette = silhouette_train\n","            best_k = k\n","            best_model = model\n","            best_train_predictions = predictions\n","\n","    # Create PySpark DataFrame with Silhouette scores\n","    spark = SparkSession.builder.getOrCreate()\n","    silhouette_df = spark.createDataFrame(silhouette_scores, schema=[\"k\", \"silhouette_score\"])\n","\n","    print(f\"Best Silhouette Score: {best_silhouette} with {best_k} clusters.\")\n","    return silhouette_df\n","\n","\n","from pyspark.sql import functions as F\n","from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n","from pyspark.ml.regression import RandomForestRegressor\n","from pyspark.sql import SparkSession\n","from pyspark.sql.window import Window\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","# 5.1. Calculate fuel consumption\n","def calculate_fuel_consumption(sample_dataset_final_with_predictions):\n","    \"\"\"\n","    Calculate fuel consumption by multiplying 'rida_per_metus' (annual mileage) with the predicted fuel consumption per 100km ('pred_kuro_sunaudojimas_l100km').\n","\n","    :param sample_dataset_final_with_predictions: PySpark DataFrame containing the 'prediction' column.\n","    :return: PySpark DataFrame with renamed 'prediction' column to 'pred_kuro_sunaudojimas_l100km'\n","             and added 'kuro_suvartojimas_lt' column for total fuel consumption.\n","    \"\"\"\n","\n","    # Rename 'prediction' column to 'pred_kuro_sunaudojimas_l100km'\n","    sample_dataset_final_with_predictions = sample_dataset_final_with_predictions.withColumnRenamed(\n","        'prediction', 'pred_kuro_sunaudojimas_l100km'\n","    )\n","\n","    # Calculate total fuel consumption by multiplying mileage by predicted fuel consumption per 100km\n","    sample_dataset_final_with_predictions = sample_dataset_final_with_predictions.withColumn(\n","        'kuro_suvartojimas_lt',\n","        (F.col('rida_per_metus') / 100) * F.col('pred_kuro_sunaudojimas_l100km')\n","    )\n","    return sample_dataset_final_with_predictions\n","\n","# 5.1.1 Total fuel consumption on sample. that has all information needed\n","def Results_sample(calculate_fuel_consumption):\n","    \"\"\"\n","    Aggregate total fuel consumption by vehicle type, vehicle description, and fuel type,\n","    and sort by vehicle type in correct numerical order.\n","\n","    :param calculate_fuel_consumption: PySpark DataFrame with calculated fuel consumption ('kuro_suvartojimas_lt').\n","    :return: Aggregated PySpark DataFrame with total fuel consumption per 'transporto_priemones_tipas',\n","             'transporto_priemones_paaiskinimas', and 'degalai'.\n","    \"\"\"\n","\n","    # Add a numeric column derived from the vehicle type for sorting\n","    kuro_suvartojimas = kuro_suvartojimas.withColumn(\n","        'transporto_priemones_tipas_skaitmenine',\n","        F.regexp_extract(F.col('transporto_priemones_tipas'), '\\d+', 0).cast(T.IntegerType())\n","    )\n","\n","    # Aggregate fuel consumption by vehicle type, description, and fuel type\n","    agreguoti_rezultatai = kuro_suvartojimas.groupBy(\n","        'transporto_priemones_tipas', 'transporto_priemones_paaiskinimas', 'degalai'\n","    ).agg(\n","        F.sum('kuro_suvartojimas_lt').alias('total_kuro_suvartojimas_lt')\n","    )\n","\n","    # Round total fuel consumption to zero decimal places\n","    agreguoti_rezultatai = agreguoti_rezultatai.withColumn(\n","        'total_kuro_suvartojimas_lt', F.round(F.col('total_kuro_suvartojimas_lt'), 0)\n","    )\n","\n","    # Sort results by numeric vehicle type and fuel type\n","    agreguoti_rezultatai = agreguoti_rezultatai.join(\n","        kuro_suvartojimas.select('transporto_priemones_tipas', 'transporto_priemones_tipas_skaitmenine').distinct(),\n","        on='transporto_priemones_tipas',\n","        how='left'\n","    ).orderBy('transporto_priemones_tipas_skaitmenine', 'degalai')\n","\n","    return agreguoti_rezultatai.drop('transporto_priemones_tipas_skaitmenine')\n","\n","# 5.1.2 Rezults on sample with additional information\n","from pyspark.sql import functions as F\n","from pyspark.sql import types as T\n","\n","def results_with_unit_avg_mileage(calculate_fuel_consumption):\n","    \"\"\"\n","    Aggregate total fuel consumption by vehicle type, description, and fuel type, including additional metrics:\n","    Number of vehicles, average fuel consumption per vehicle, average mileage per year\n","\n","    :param calculate_fuel_consumption: PySpark DataFrame with calculated fuel consumption and predictions.\n","    :return: Aggregated PySpark DataFrame with total fuel consumption, vehicle count,\n","             average fuel consumption, and average mileage.\n","    \"\"\"\n","\n","    # Add a numeric column derived from the vehicle type for sorting\n","    kuro_suvartojimas = kuro_suvartojimas.withColumn(\n","        'transporto_priemones_tipas_skaitmenine',\n","        F.regexp_extract(F.col('transporto_priemones_tipas'), '\\d+', 0).cast(T.IntegerType())\n","    )\n","\n","    # Aggregate fuel consumption, vehicle count, and averages\n","    agreguoti_rezultatai = kuro_suvartojimas.groupBy(\n","        'transporto_priemones_tipas', 'transporto_priemones_paaiskinimas', 'degalai'\n","    ).agg(\n","        F.sum('kuro_suvartojimas_lt').alias('total_kuro_suvartojimas_lt'),\n","        F.count('transporto_priemones_tipas').alias('transporto_priemoniu_kiekis'),\n","        F.avg('pred_kuro_sunaudojimas_l100km').alias('vid_kuro_sunaudojimas_100km'),\n","        F.avg('rida_per_metus').alias('vid_rida_per_metus')\n","    )\n","\n","    # Round results for better readability\n","    agreguoti_rezultatai = agreguoti_rezultatai.withColumn(\n","        'total_kuro_suvartojimas_lt', F.round(F.col('total_kuro_suvartojimas_lt'), 0)\n","    ).withColumn(\n","        'vid_kuro_sunaudojimas_100km', F.round(F.col('vid_kuro_sunaudojimas_100km'), 2)\n","    ).withColumn(\n","        'vid_rida_per_metus', F.round(F.col('vid_rida_per_metus'), 0)\n","    )\n","\n","    # Sort results by numeric vehicle type and fuel type\n","    agreguoti_rezultatai = agreguoti_rezultatai.join(\n","        kuro_suvartojimas.select('transporto_priemones_tipas', 'transporto_priemones_tipas_skaitmenine').distinct(),\n","        on='transporto_priemones_tipas',\n","        how='left'\n","    ).orderBy('transporto_priemones_tipas_skaitmenine', 'degalai')\n","\n","    return agreguoti_rezultatai.drop('transporto_priemones_tipas_skaitmenine')\n","\n","def proportions_on_fuel(regitra_transeksta_final):\n","    \"\"\"\n","    Calculates proportions of vehicle records by fuel type and vehicle type for the entire dataset.\n","\n","    :param regitra_transeksta_final: PySpark DataFrame containing vehicle data.\n","    :return: PySpark DataFrame with proportions calculated for each fuel type and vehicle type.\n","    \"\"\"\n","    # Remove records where 'transporto_priemones_tipas' is NULL\n","    regitra_transeksta_final = regitra_transeksta_final.filter(F.col('transporto_priemones_tipas').isNotNull())\n","\n","    # Calculate the total number of records\n","    total_count = regitra_transeksta_final.count()\n","\n","    # Calculate the number of records where 'rida_per_metus' is not NULL\n","    count_with_rida = regitra_transeksta_final.filter(F.col('rida_per_metus').isNotNull()).count()\n","\n","    # Group by vehicle type, vehicle description, and fuel type, and count records\n","    proportions_df = regitra_transeksta_final.groupBy('transporto_priemones_tipas', 'transporto_priemones_paaiskinimas', 'degalai') \\\n","        .agg(\n","            F.count('*').alias('total_count'),\n","            F.count(F.when(F.col('rida_per_metus').isNotNull(), 1)).alias('count_with_rida')\n","        )\n","\n","    # Calculate proportions as percentages with four decimal places\n","    proportions_df = proportions_df.withColumn(\n","        'proportion_total',\n","        F.round((F.col('total_count') / total_count) * 100, 4)\n","    ).withColumn(\n","        'proportion_with_rida',\n","        F.round((F.col('count_with_rida') / count_with_rida) * 100, 4)\n","    )\n","\n","    # Add aggregate row ('K0') for each fuel type\n","    total_rows = regitra_transeksta_final.groupBy('degalai') \\\n","        .agg(\n","            F.count('*').alias('total_count'),\n","            F.count(F.when(F.col('rida_per_metus').isNotNull(), 1)).alias('count_with_rida')\n","        ) \\\n","        .withColumn('transporto_priemones_tipas', F.lit('K0')) \\\n","        .withColumn('transporto_priemones_paaiskinimas', F.lit('Iš viso'))\n","\n","    # Calculate proportions for total rows\n","    total_rows = total_rows.withColumn(\n","        'proportion_total',\n","        F.round((F.col('total_count') / total_count) * 100, 4)\n","    ).withColumn(\n","        'proportion_with_rida',\n","        F.round((F.col('count_with_rida') / count_with_rida) * 100, 4)\n","    )\n","\n","    # Combine individual proportions with total rows\n","    proportions_df = proportions_df.unionByName(total_rows)\n","\n","    # Sort the final result by vehicle type and fuel type\n","    proportions_df = proportions_df.orderBy('transporto_priemones_tipas', 'degalai')\n","\n","    return proportions_df\n","\n","# Final fuel consumption on Lithuania fleet\n","from pyspark.sql import functions as F\n","\n","def calculate_final_fuel_consumption(results_with_unit_avg_mileage, proportions_on_fuel):\n","      \"\"\"\n","    Adjusts fuel consumption sample proportions to match population proportions and calculates the total fuel consumption for the population.\n","\n","    :param results_with_unit_avg_mileage: PySpark DataFrame with sample data including fuel consumption and vehicle counts.\n","    :param proportions_on_fuel: PySpark DataFrame with population proportions for vehicle and fuel types.\n","    :return: PySpark DataFrame with adjusted fuel consumption for the population.\n","    \"\"\"\n","\n","    # Select relevant columns and exclude 'K0' aggregate row\n","    Proporcijos_ant_degalu = Proporcijos_ant_degalu.select(\n","        'transporto_priemones_tipas', 'degalai', 'total_count', 'proportion_total'\n","    ).filter(F.col('transporto_priemones_tipas') != 'K0')\n","\n","    # Convert population proportions to a range of 0-1\n","    Proporcijos_ant_degalu = Proporcijos_ant_degalu.withColumn(\n","        'proportion_total', F.col('proportion_total') / 100\n","    )\n","\n","    # Calculate the total number of vehicles in the sample\n","    imtis_sum = Rezultatai_su_vnt_vid_rida.agg(F.sum('transporto_priemoniu_kiekis').alias('total_imties_kiekis')).collect()[0][0]\n","\n","    # Calculate sample proportions\n","    imties_proporcijos = Rezultatai_su_vnt_vid_rida.withColumn(\n","        'imties_proporcija',\n","        F.col('transporto_priemoniu_kiekis') / imtis_sum\n","    )\n","\n","    # Join sample proportions with population proportions\n","    sujungti_duomenys = imties_proporcijos.join(\n","        Proporcijos_ant_degalu,\n","        on=['transporto_priemones_tipas', 'degalai'],\n","        how='right'\n","    )\n","\n","    # Fill NULL values with defaults\n","    sujungti_duomenys = sujungti_duomenys.fillna({\n","        'transporto_priemoniu_kiekis': 0,\n","        'total_kuro_suvartojimas_lt': 0,\n","        'imties_proporcija': 0\n","    })\n","\n","    # Filter rows with vehicle counts greater than zero\n","    sujungti_duomenys = sujungti_duomenys.filter(F.col('transporto_priemoniu_kiekis') > 0)\n","\n","    # Adjust vehicle counts to match population proportions\n","    sujungti_duomenys = sujungti_duomenys.withColumn(\n","        'koreguotas_transporto_priemoniu_kiekis',\n","        F.col('proportion_total') * imtis_sum\n","    )\n","\n","    # Adjust fuel consumption to align with new proportions\n","    sujungti_duomenys = sujungti_duomenys.withColumn(\n","        'koreguotas_kuro_suvartojimas',\n","        F.col('total_kuro_suvartojimas_lt') * (F.col('proportion_total') / F.col('imties_proporcija'))\n","    )\n","\n","    # Calculate scaling factor to match population totals\n","    sujungti_duomenys = sujungti_duomenys.withColumn(\n","        'padidinimo_koeficientas',\n","        F.col('total_count') / F.col('koreguotas_transporto_priemoniu_kiekis')\n","    )\n","\n","    # Adjust fuel consumption by scaling factor\n","    sujungti_duomenys = sujungti_duomenys.withColumn(\n","        'galutinis_kuro_suvartojimas_populiacijai',\n","        F.col('koreguotas_kuro_suvartojimas') * F.col('padidinimo_koeficientas')\n","    )\n","\n","    # Select final columns for the output\n","    galutiniai_rezultatai = sujungti_duomenys.select(\n","        'transporto_priemones_tipas',\n","        'degalai',\n","        'koreguotas_transporto_priemoniu_kiekis',\n","        'koreguotas_kuro_suvartojimas',\n","        'padidinimo_koeficientas',\n","        'galutinis_kuro_suvartojimas_populiacijai'\n","    )\n","\n","    return galutiniai_rezultatai\n"],"metadata":{"id":"FayFK87xo4QE"},"execution_count":null,"outputs":[]}]}