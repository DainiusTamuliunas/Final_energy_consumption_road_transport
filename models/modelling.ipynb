{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMvfXOioRaAPQaBHqhH6lCq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xfJq-v2bBrS4"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["pip install catboost"],"metadata":{"id":"YTVvBittp6Bh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt"],"metadata":{"id":"WkPDv5N0DeMf","executionInfo":{"status":"ok","timestamp":1736124398837,"user_tz":-120,"elapsed":9182,"user":{"displayName":"Dainius Tamuliūnas","userId":"04370657899948480821"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["file_path = '/content/drive/MyDrive/Master Paper/Data/'\n","\n","df = pd.read_csv(file_path + 'eu_total_final.csv')\n","\n","# Feature engineering for K6 and K2\n","df['mase'] = np.where(\n","    df['transporto_priemones_tipas'] == 'K6',\n","    df['maksimali_mase_kg'],\n","    df['nuosava_mase_kg']\n",")\n","\n","# Group by 'transporto_priemones_tipas' and calculate the value counts of 'degalai'\n","counts = df.groupby('transporto_priemones_tipas')['degalai'].value_counts()\n","\n","# Convert the result to a DataFrame for better readability (optional)\n","counts_df = counts.reset_index(name='count')\n","\n","print(counts_df)\n"],"metadata":{"id":"kmcEhw0bCnia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Group by 'transporto_priemones_tipas' and calculate the value counts of 'degalai'\n","counts = df.groupby('transporto_priemones_tipas')['degalai'].value_counts()\n","\n","# Convert the result to a DataFrame for better readability (optional)\n","counts_df = counts.reset_index(name='count')\n","\n","print(counts_df)\n"],"metadata":{"id":"sosXGkOQZpQx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.linear_model import LinearRegression, Lasso, BayesianRidge, Ridge, ElasticNet\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from sklearn.svm import SVR\n","from sklearn.pipeline import Pipeline\n","import matplotlib.pyplot as plt\n","import xgboost as xgb\n","from lightgbm import LGBMRegressor\n","from catboost import CatBoostRegressor\n","from sklearn.impute import SimpleImputer\n","\n","# Feature engineering for K6 and K2\n","dfs['mase'] = np.where(\n","    dfs['transporto_priemones_tipas'] == 'K6',\n","    dfs['maksimali_mase_kg'],\n","    dfs['nuosava_mase_kg']\n",")\n","\n","# Load and preprocess data\n","features = ['variklio_turis_cm3', 'galia_kw', 'degalai', 'transporto_priemones_tipas', 'mase']\n","target = 'kuro_sunaudojimas_l100km_org'\n","\n","df_filtered = df[features + [target]].copy()\n","\n","# Handle missing values\n","imputer = SimpleImputer(strategy='most_frequent')  # For both numerical and categorical\n","df_filtered[features] = imputer.fit_transform(df_filtered[features])\n","df_filtered[target] = df_filtered[target].fillna(df_filtered[target].mean())\n","\n","# Split data\n","X = df_filtered[features]\n","y = df_filtered[target]\n","# Combine stratification columns into a single column\n","X['stratify_group'] = X['transporto_priemones_tipas'].astype(str) + \"_\" + X['degalai'].astype(str)\n","\n","# Perform stratified split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X.drop(columns=['stratify_group']),  # Drop the stratification column from features\n","    y,\n","    test_size=0.2,\n","    random_state=42,\n","    stratify=X['stratify_group']\n",")\n","\n","# Preprocessing: Scaling and Encoding\n","numeric_features = ['variklio_turis_cm3', 'galia_kw', 'mase']\n","categorical_features = ['degalai', 'transporto_priemones_tipas']\n","\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', StandardScaler(), numeric_features),\n","        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n","    ])\n","\n","# Define result path\n","result_path = '/content/drive/MyDrive/Master Paper/Results/'\n","os.makedirs(result_path, exist_ok=True)\n","\n","# Function for evaluation metrics\n","def evaluate_model(model, X_test, y_test, name):\n","    y_pred = model.predict(X_test)\n","    mae = mean_absolute_error(y_test, y_pred)\n","    mse = mean_squared_error(y_test, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_test, y_pred)\n","    print(f\"{name} - MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n","    return mae, rmse, r2\n","\n","# Hyperparameter tuning using GridSearchCV\n","def tune_model(model, param_grid, X_train, y_train):\n","    grid_search = GridSearchCV(\n","        model,\n","        param_grid,\n","        scoring='neg_mean_absolute_error',\n","        cv=5,\n","        n_jobs=-1,\n","        verbose=1\n","    )\n","    grid_search.fit(X_train, y_train)\n","    print(f\"Best Parameters: {grid_search.best_params_}\")\n","    return grid_search.best_estimator_\n","\n","\n","\n","def save_model_results(model, model_name, X_test_transformed, y_test, X_test):\n","    \"\"\"\n","    Evaluate the model, save evaluation metrics, and create prediction graphs.\n","    \"\"\"\n","    # Predict\n","    y_pred = model.predict(X_test_transformed)\n","\n","    # Evaluate the model\n","    mae, rmse, r2 = evaluate_model(model, X_test_transformed, y_test, model_name)\n","\n","    # Save evaluation metrics to CSV\n","    metrics = {\n","        'Model': model_name,\n","        'MAE': mae,\n","        'RMSE': rmse,\n","        'R²': r2\n","    }\n","    metrics_df = pd.DataFrame([metrics])\n","    metrics_csv_path = os.path.join(result_path, 'model_evaluation_metrics.csv')\n","\n","    if os.path.exists(metrics_csv_path):\n","        metrics_df.to_csv(metrics_csv_path, mode='a', header=False, index=False)\n","    else:\n","        metrics_df.to_csv(metrics_csv_path, index=False)\n","\n","    # Save prediction graphs for each transport type\n","    unique_transport_types = X_test['transporto_priemones_tipas'].unique()\n","    model_folder = os.path.join(result_path, model_name)\n","    os.makedirs(model_folder, exist_ok=True)\n","\n","    for transport_type in unique_transport_types:\n","        transport_df = X_test[X_test['transporto_priemones_tipas'] == transport_type]\n","        fuel_types = transport_df['degalai'].unique()\n","\n","        plt.figure(figsize=(8, 6))\n","        for fuel_type in fuel_types:\n","            fuel_df = transport_df[transport_df['degalai'] == fuel_type]\n","\n","            # Map fuel_df indices to positions in y_pred\n","            pred_positions = fuel_df.index.map(lambda idx: X_test.index.get_loc(idx))\n","            y_pred_group = y_pred[pred_positions]\n","            y_test_group = y_test.loc[fuel_df.index]\n","\n","            # Translate fuel type to English\n","            fuel_type_label = \"Diesel\" if fuel_type == \"Dyzelinas\" else \"Petrol\"\n","\n","            # Plot predicted vs. actual\n","            plt.scatter(\n","                y_pred_group,\n","                y_test_group,\n","                label=f\"{fuel_type_label}\",\n","                alpha=0.5\n","            )\n","\n","        plt.xlabel(\"Predicted Fuel Consumption (L/100km)\")\n","        plt.ylabel(\"Actual Fuel Consumption (L/100km)\")\n","        plt.xlim(3, 15)  # Set X-axis limits\n","        plt.ylim(3, 15)  # Set Y-axis limits\n","        plt.legend(title=\"Fuel Type\")\n","        plt.grid(True)\n","\n","        # Save the graph\n","        plt_path = os.path.join(model_folder, f\"{transport_type}_predicted_vs_actual.png\")\n","        plt.savefig(plt_path)\n","        plt.close()\n","\n","# Linear Regression\n","lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', LinearRegression())])\n","lr_pipeline.fit(X_train, y_train)  # Train the model\n","save_model_results(lr_pipeline, \"Linear Regression\", X_test, y_test, X_test)\n","\n","# Ridge Regression\n","ridge_param_grid = {'model__alpha': np.arange(0.05, 1.05, 0.05)}\n","ridge_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', Ridge())])\n","ridge_best = tune_model(ridge_pipeline, ridge_param_grid, X_train, y_train)\n","save_model_results(ridge_best, \"Ridge Regression\", X_test, y_test, X_test)\n","\n","# Lasso Regression\n","lasso_param_grid = {'model__alpha': np.arange(0.05, 1.05, 0.05)}\n","lasso_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', Lasso())])\n","lasso_best = tune_model(lasso_pipeline, lasso_param_grid, X_train, y_train)\n","save_model_results(lasso_best, \"Lasso Regression\", X_test, y_test, X_test)\n","\n","# ElasticNet Regression\n","elasticnet_param_grid = {\n","    'model__alpha': np.arange(0.05, 1.05, 0.05),\n","    'model__l1_ratio': np.arange(0.1, 1.1, 0.1)\n","}\n","elasticnet_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', ElasticNet(max_iter=1000))])\n","elasticnet_best = tune_model(elasticnet_pipeline, elasticnet_param_grid, X_train, y_train)\n","save_model_results(elasticnet_best, \"ElasticNet Regression\", X_test, y_test, X_test)\n","\n","# Gradient Boosting\n","gbm_param_grid = {\n","    'model__n_estimators': [25, 100, 200],\n","    'model__learning_rate': [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.2],\n","    'model__max_depth': [2, 3, 4]\n","}\n","gbm_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', GradientBoostingRegressor())])\n","gbm_best = tune_model(gbm_pipeline, gbm_param_grid, X_train, y_train)  # Tune and train\n","save_model_results(gbm_best, \"Gradient Boosting\", X_test, y_test, X_test)  # Save results\n","\n","# XGBoost Manual Grid Search with Plotting\n","best_params = None\n","best_score = float('inf')\n","\n","# Dictionary to store evaluation results\n","xgb_evaluation_metrics = []\n","\n","# Preprocess the data\n","preprocessor.fit(X_train)\n","\n","X_train_transformed = preprocessor.transform(X_train)\n","X_test_transformed = preprocessor.transform(X_test)\n","\n","# Continue with XGBoost training\n","for n_estimators in [25, 100, 200, 400]:\n","    for learning_rate in [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.2]:\n","        for max_depth in [2, 3, 4]:\n","            model = xgb.XGBRegressor(\n","                n_estimators=n_estimators,\n","                learning_rate=learning_rate,\n","                max_depth=max_depth,\n","                random_state=42\n","            )\n","            model.fit(X_train_transformed, y_train)\n","            y_pred = model.predict(X_test_transformed)\n","            score = mean_absolute_error(y_test, y_pred)\n","            xgb_evaluation_metrics.append({\n","                \"n_estimators\": n_estimators,\n","                \"learning_rate\": learning_rate,\n","                \"max_depth\": max_depth,\n","                \"MAE\": score\n","            })\n","            if score < best_score:\n","                best_score = score\n","                best_params = {\n","                    \"n_estimators\": n_estimators,\n","                    \"learning_rate\": learning_rate,\n","                    \"max_depth\": max_depth,\n","                }\n","\n","print(\"Best XGBoost Params:\", best_params)\n","\n","# Train the Best XGBoost Model\n","best_xgb_model = xgb.XGBRegressor(\n","    n_estimators=best_params[\"n_estimators\"],\n","    learning_rate=best_params[\"learning_rate\"],\n","    max_depth=best_params[\"max_depth\"],\n","    random_state=42\n",")\n","best_xgb_model.fit(X_train_transformed, y_train)\n","save_model_results(best_xgb_model, \"XGBoost\", X_test_transformed, y_test, X_test)\n","\n","\n","# LightGBM\n","lgb_param_grid = {\n","    'model__n_estimators': [25, 100, 200, 400, 1000],\n","    'model__learning_rate': [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.5, 1],\n","    'model__max_depth': [2, 3, 4, 5]\n","}\n","lgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', LGBMRegressor())])\n","lgb_best = tune_model(lgb_pipeline, lgb_param_grid, X_train, y_train)\n","save_model_results(lgb_best, \"LightGBM\", X_test, y_test, X_test)\n","\n","# CatBoost\n","catboost_param_grid = {\n","    'model__iterations': [50, 100, 200, 400, 1000],\n","    'model__depth': [2, 3, 4, 6],\n","    'model__learning_rate': [ 0.05, 0.1, 0.2, 0.5, 1,],\n","}\n","\n","catboost_model = CatBoostRegressor(random_state=42, silent=True)\n","catboost_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', catboost_model)])\n","catboost_best = tune_model(catboost_pipeline, catboost_param_grid, X_train, y_train)\n","save_model_results(catboost_best, \"CatBoost\", X_test, y_test, X_test)\n","\n","# Support Vector Machine Regressor (SVR)\n","svr_param_grid = {\n","    'model__C': [10, 100, 200],\n","    'model__epsilon': [ 0.1, 0.2, 0.5]\n","}\n","\n","svr_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', SVR(kernel='rbf'))])\n","svr_best = tune_model(svr_pipeline, svr_param_grid, X_train, y_train)\n","save_model_results(svr_best,  \"SVR\", X_test, y_test, X_test)  # Save results\n","\n","# Define the parameter grid for Random Forest\n","rf_param_grid = {\n","    'model__n_estimators': [200, 400],        # Number of trees in the forest\n","    'model__max_depth': [None, 10, 20],          # Maximum depth of the tree\n","    'model__min_samples_split': [2, 5, 10, 20],      # Minimum number of samples required to split an internal node\n","    'model__min_samples_leaf': [1, 2, 4, 10]         # Minimum number of samples required to be at a leaf node\n","}\n","\n","\n","# Create a Random Forest pipeline\n","rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', RandomForestRegressor(random_state=42))])\n","\n","# Tune the Random Forest model using the parameter grid\n","rf_best = tune_model(rf_pipeline, rf_param_grid, X_train, y_train)\n","\n","# Save the results of the best model\n","save_model_results(rf_best, \"Random Forest\", X_test, y_test, X_test)\n","\n"],"metadata":{"id":"5jfj980NfAsh"},"execution_count":null,"outputs":[]}]}