{"cells":[{"cell_type":"code","source":["# Install necessary libraries with specific versions\n","!pip install -qU \\\n","    langchain==0.0.354 \\\n","    openai==1.6.1 \\\n","    datasets==2.10.1 \\\n","    pinecone-client==3.1.0 \\\n","    tiktoken==0.5.2 \\\n","    python-Levenshtein \\\n","    unidecode\n","\n","# Import the Google Drive library for Colab and mount the drive to access files\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","import re\n","from Levenshtein import distance as levenshtein_distance\n","import tiktoken\n","from concurrent.futures import ThreadPoolExecutor\n","from langchain.schema import HumanMessage\n","\n","from langchain.chat_models import ChatOpenAI\n","import pandas as pd\n","chat = ChatOpenAI(\n","    openai_api_key='---', # openai_api_key is a private key, because of that it is deleted\n","    model='gpt-3.5-turbo'\n",")\n","\n","# Initialize tokenizer\n","enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n","\n","def load_dataset(file_path):\n","    \"\"\"\n","    Load a dataset from the given file path.\n","\n","    :param file_path: Path to the dataset file.\n","    :return: Loaded pandas DataFrame.\n","    \"\"\"\n","    if file_path.endswith('.xlsx'):\n","        return pd.read_excel(file_path)\n","    elif file_path.endswith('.csv'):\n","        return pd.read_csv(file_path)\n","    else:\n","        raise ValueError(\"Unsupported file format. Use .csv or .xlsx\")\n","\n","def clean_lithuanian_dataset(df, drop_columns=None):\n","    \"\"\"\n","    Clean the Lithuanian dataset by dropping unnecessary columns, normalizing text,\n","    replacing Lithuanian letters, and removing unwanted symbols.\n","\n","    :param df: Pandas DataFrame to clean.\n","    :param drop_columns: List of columns to drop.\n","    :return: Cleaned DataFrame.\n","    \"\"\"\n","    if drop_columns:\n","        df = df.drop(columns=drop_columns)\n","\n","    lithuanian_to_english_mapping = {\n","        'ą': 'a', 'č': 'c', 'ę': 'e', 'ė': 'e', 'į': 'i',\n","        'š': 's', 'ū': 'u', 'ž': 'z', 'ô': 'o', 'Ų': 'U'\n","    }\n","\n","    def replace_lithuanian_letters(text):\n","        if pd.isna(text):\n","            return text\n","        for lith_letter, eng_letter in lithuanian_to_english_mapping.items():\n","            text = text.replace(lith_letter, eng_letter)\n","        return text\n","\n","    def clean_symbols(text):\n","        if pd.isna(text):\n","            return text\n","        return re.sub(r\"[^A-Za-z0-9\\s,]\", \"\", text).replace(\"-\", \" \")\n","\n","    # Normalize columns\n","    df['marke'] = df['marke'].str.lower().str.strip().apply(replace_lithuanian_letters).apply(clean_symbols)\n","    df['modelis'] = df['modelis'].str.lower().str.strip().apply(replace_lithuanian_letters).apply(clean_symbols)\n","    return df\n","\n","def clean_european_dataset(df_eu):\n","    \"\"\"\n","    Clean the European dataset by standardizing columns and filtering uninformative rows.\n","\n","    :param df_eu: Pandas DataFrame to clean.\n","    :return: Cleaned DataFrame.\n","    \"\"\"\n","    df_eu['marke'] = df_eu['marke'].str.lower().str.strip()\n","    df_eu['modelis'] = df_eu['modelis'].str.lower().str.strip()\n","\n","    # Filter out uninformative rows from 'modelis'\n","    condition = (df_eu['modelis'].isna() |\n","                 df_eu['modelis'].str.strip().isin(['', '-', '--', '---', 'NaN']) |\n","                 df_eu['modelis'].str.strip().apply(lambda x: isinstance(x, str) and all(c == '-' for c in x)))\n","    df_eu = df_eu[~condition]\n","    return df_eu\n","\n","def process_columns(df):\n","    \"\"\"\n","    Explode 'marke' and 'modelis' columns and clean them.\n","\n","    :param df: Pandas DataFrame to process.\n","    :return: Processed DataFrame.\n","    \"\"\"\n","    df_split = df.assign(\n","        marke=df['marke'].str.split(','),\n","        modelis=df['modelis'].str.split(',')\n","    )\n","    df_exploded = df_split.explode('marke').explode('modelis')\n","    df_exploded['marke'] = df_exploded['marke'].str.strip().str.replace(r\"[\\'\\\"]\", \" \", regex=True).str.lower()\n","    df_exploded['modelis'] = df_exploded['modelis'].str.strip().str.replace(r\"[\\'\\\"]\", \" \", regex=True).str.lower()\n","    df_exploded.drop_duplicates(inplace=True)\n","    return df_exploded\n","\n","def calculate_tokens_for_row(marke, modelis, enc):\n","    \"\"\"\n","    Calculate the number of tokens required for a single row, including formatting.\n","\n","    :param marke: The car maker's name as a string.\n","    :param modelis: The car model's name as a string.\n","    :param enc: The tokenizer object initialized for GPT-3.5.\n","    :return: The number of tokens required for the row.\n","    \"\"\"\n","    row_string = f\"Marke: {marke}\\nModelis: {modelis}\\n\\n\"\n","    tokens = enc.encode(row_string)\n","    return len(tokens)\n","\n","def fine_tuned_prompt():\n","    \"\"\"\n","    Generate a pre-defined prompt for GPT-based cleaning to ensure consistent instructions.\n","\n","    :return: A string containing the formatted instructions for GPT.\n","    \"\"\"\n","    return (\"\"\"Follow these instructions exactly:\n","\n","        1. For each row, provide the car maker and car model in this format:\n","           Original Marke: [original value]\n","           Original Modelis: [original value]\n","           Fixed Marke: [corrected value]\n","           Fixed Modelis: [corrected value]\n","           - 'Original Marke' and 'Original Modelis' should match exactly what was provided in the input.\n","           - 'Fixed Marke' should contain only the corrected car maker's name.\n","           - 'Fixed Modelis' should contain the corrected car model, without repeating the car maker.\n","\n","        2. Fix any spelling or grammatical errors in both 'Fixed Marke' and 'Fixed Modelis' to reflect real car makers and models.\n","\n","        3. Expand abbreviations in 'Fixed Marke' (e.g., \"MB\" becomes \"Mercedes-Benz\", \"VW\" becomes \"Volkswagen\").\n","\n","        4. If multiple models are listed in 'Original Modelis', keep only the first one in 'Fixed Modelis'.\n","\n","        5. Ensure that the car maker and model are real (e.g., BMW X5, Audi A100).\n","\n","        6. Separate each row with '---'.\"\"\")\n","\n","def calculate_tokens_for_batch(batch_df, enc):\n","    \"\"\"\n","    Calculate the total number of tokens required for a batch of rows, including the prompt.\n","\n","    :param batch_df: A pandas DataFrame containing 'marke' and 'modelis' columns.\n","    :param enc: The tokenizer object initialized for GPT-3.5.\n","    :return: The total number of tokens required for the batch.\n","    \"\"\"\n","    prompt = fine_tuned_prompt()\n","    token_count = len(enc.encode(prompt))\n","    for _, row in batch_df.iterrows():\n","        marke = row['marke'] if pd.notna(row['marke']) else \"\"\n","        modelis = row['modelis'] if pd.notna(row['modelis']) else \"\"\n","        token_count += calculate_tokens_for_row(marke, modelis, enc)\n","\n","    return token_count\n","\n","def gpt_clean_batch_dynamic(batch_df):\n","    \"\"\"\n","    Process a batch of rows with GPT to clean and correct car maker and model names.\n","\n","    :param batch_df: A pandas DataFrame containing rows to be cleaned.\n","    :return: A list of tuples with original and corrected 'marke' and 'modelis'.\n","    \"\"\"\n","    prompt = fine_tuned_prompt()\n","\n","    # Constructing the prompt by iterating over the batch_df\n","    for _, row in batch_df.iterrows():\n","        marke = row['marke'] if pd.notna(row['marke']) else \"\"\n","        modelis = row['modelis'] if pd.notna(row['modelis']) else \"\"\n","        prompt += f\"\\nOriginal Marke: {marke}\\nOriginal Modelis: {modelis}\\n---\"\n","\n","    prompt_message = HumanMessage(content=prompt)\n","    messages = [prompt_message]\n","\n","    try:\n","        response = chat(messages)  # Your function with API key is called here\n","        cleaned_response = response.content.strip().split(\"---\")\n","\n","        cleaned_data = []\n","        for cleaned_row in cleaned_response:\n","            # Split rows into `original` and `fixed` values\n","            marke, modelis, fixed_marke, fixed_modelis = None, None, None, None\n","            for line in cleaned_row.split('\\n'):\n","                if line.startswith(\"Original Marke:\"):\n","                    marke = line.replace(\"Original Marke:\", \"\").strip()\n","                elif line.startswith(\"Original Modelis:\"):\n","                    modelis = line.replace(\"Original Modelis:\", \"\").strip()\n","                elif line.startswith(\"Fixed Marke:\"):\n","                    fixed_marke = line.replace(\"Fixed Marke:\", \"\").strip()\n","                elif line.startswith(\"Fixed Modelis:\"):\n","                    fixed_modelis = line.replace(\"Fixed Modelis:\", \"\").strip()\n","\n","            # Append only if both original and fixed values are available\n","            if marke is not None and modelis is not None and fixed_marke is not None and fixed_modelis is not None:\n","                cleaned_data.append((marke, modelis, fixed_marke, fixed_modelis))\n","\n","        return cleaned_data\n","\n","    except Exception as e:\n","        print(f\"Error cleaning batch with GPT: {e}\")\n","        # Return an empty list to indicate failure for this batch\n","        return []\n","\n","# Function to clean the dataframe in parallel, ensuring token limits are respected\n","def clean_dataframe_in_batches_parallel(df, enc, max_tokens=1000, save_interval=100000, output_file=\"cleaned_results.csv\"):\n","    \"\"\"\n","    Clean a DataFrame in parallel using GPT-based processing, with intermediate results saved periodically.\n","\n","    :param df: A pandas DataFrame containing 'marke' and 'modelis' columns.\n","    :param enc: The tokenizer object initialized for GPT-3.5 or compatible model.\n","    :param max_tokens: Maximum token limit for each batch (default: 1000).\n","    :param save_interval: Number of rows after which to save intermediate results (default: 100000).\n","    :param output_file: Path to save intermediate and final results.\n","    :return: A cleaned pandas DataFrame.\n","    \"\"\"\n","    cleaned_data = []\n","    start_idx = 0\n","    num_rows = len(df)\n","    last_saved_idx = 0\n","\n","    with ThreadPoolExecutor(max_workers=5) as executor:\n","        futures = []\n","\n","        while start_idx < num_rows:\n","            end_idx = start_idx + 1\n","\n","            while end_idx <= num_rows:\n","                batch_df = df.iloc[start_idx:end_idx]\n","                token_count = calculate_tokens_for_batch(batch_df, enc)\n","                if token_count > max_tokens:\n","                    break\n","                end_idx += 1\n","\n","            batch_df = df.iloc[start_idx:end_idx-1]\n","            futures.append(executor.submit(gpt_clean_batch_dynamic, batch_df))\n","\n","            print(f\"Processing batch from row {start_idx} to {end_idx-1} with {token_count} tokens\")\n","\n","            start_idx = end_idx - 1\n","\n","            # Save the results after processing every save_interval rows\n","            if start_idx - last_saved_idx >= save_interval:\n","                for future in futures:\n","                    cleaned_batch = future.result()\n","                    cleaned_data.extend(cleaned_batch)\n","\n","                # Convert cleaned data to DataFrame and save\n","                cleaned_df = pd.DataFrame(cleaned_data, columns=['marke', 'modelis', 'cleaned_marke', 'cleaned_modelis'])\n","                cleaned_df.to_csv(output_file, mode='a', index=False, header=(last_saved_idx == 0))  # Append to CSV\n","                last_saved_idx = start_idx\n","                print(f\"Intermediate results saved at row {last_saved_idx}.\")\n","\n","    # After the loop, process any remaining data\n","    for future in futures:\n","        cleaned_batch = future.result()\n","        cleaned_data.extend(cleaned_batch)\n","\n","    cleaned_df = pd.DataFrame(cleaned_data, columns=['marke', 'modelis', 'cleaned_marke', 'cleaned_modelis'])\n","    return cleaned_df\n","\n","def add_levenshtein_distance(df):\n","    \"\"\"\n","    Calculate the Levenshtein distance between original and cleaned 'marke' values\n","    and store it in a new column.\n","\n","    :param df: A pandas DataFrame containing 'marke' and 'cleaned_marke' columns.\n","    :return: The DataFrame with an added 'lev_distance' column.\n","    \"\"\"\n","    df['lev_distance'] = df.apply(lambda row: levenshtein_distance(str(row['marke']), str(row['cleaned_marke'])), axis=1)\n","    return df\n","\n","def is_lowercase(s):\n","    \"\"\"\n","    Check if a string is entirely lowercase.\n","\n","    :param s: A string to check.\n","    :return: True if the string is entirely lowercase, False otherwise.\n","    \"\"\"\n","    if isinstance(s, str):\n","        return s == s.lower()\n","    return False\n","\n","def save_dataset(df, output_path):\n","    \"\"\"\n","    Save the cleaned DataFrame to a CSV file.\n","\n","    :param df: Pandas DataFrame to save.\n","    :param output_path: File path to save the DataFrame.\n","    \"\"\"\n","    df.to_csv(output_path, index=False)\n","\n","def chatgpt_cleaning_pipeline(file_path, output_path, dataset_type, drop_columns=None, gpt_output_file=None, save_interval=100000):\n","    \"\"\"\n","    Run the cleaning pipeline for the specified dataset, including text normalization,\n","    GPT-based cleaning, and quality checks like Levenshtein distance.\n","\n","    :param file_path: Path to the input dataset.\n","    :param output_path: Path to save the cleaned dataset.\n","    :param dataset_type: Type of dataset ('lithuanian' or 'european').\n","    :param drop_columns: List of columns to drop (only for Lithuanian dataset).\n","    :param gpt_output_file: Temporary file path to save intermediate GPT-cleaned results.\n","    :param save_interval: Number of rows after which to save intermediate results.\n","    \"\"\"\n","    print(f\"Loading dataset from {file_path}...\")\n","    df = load_dataset(file_path)\n","\n","    # Dataset-specific cleaning\n","    if dataset_type == 'lithuanian':\n","        print(\"Cleaning Lithuanian dataset...\")\n","        df = clean_lithuanian_dataset(df, drop_columns)\n","    elif dataset_type == 'european':\n","        print(\"Cleaning European dataset...\")\n","        df = clean_european_dataset(df)\n","    else:\n","        raise ValueError(\"Invalid dataset type. Use 'lithuanian' or 'european'.\")\n","\n","    # Column processing\n","    print(\"Processing 'marke' and 'modelis' columns...\")\n","    df_processed = process_columns(df)\n","\n","    # GPT-based cleaning\n","    print(\"Cleaning dataset with GPT-3.5...\")\n","    df_gpt_cleaned = clean_dataframe_in_batches_parallel(df_processed, enc, save_interval=save_interval, output_file=gpt_output_file)\n","\n","    # Levenshtein distance calculation\n","    print(\"Calculating Levenshtein distance...\")\n","    df_gpt_cleaned = add_levenshtein_distance(df_gpt_cleaned)\n","\n","    # Identify mismatched rows\n","    print(\"Identifying mismatched rows...\")\n","    mismatched_rows = df_gpt_cleaned[\n","        (df_gpt_cleaned['lev_distance'] > 5) |\n","        (df_gpt_cleaned['cleaned_marke'].apply(lambda s: isinstance(s, str) and s == s.lower())) |\n","        (df_gpt_cleaned['cleaned_modelis'].apply(lambda s: isinstance(s, str) and s == s.lower()))\n","    ]\n","    print(f\"Number of mismatched rows: {len(mismatched_rows)}\")\n","\n","    # Rerun GPT for mismatched rows if any\n","    if len(mismatched_rows) > 0:\n","        print(\"Re-cleaning mismatched rows with GPT...\")\n","        corrected_rows = clean_dataframe_in_batches_parallel(mismatched_rows, enc, max_tokens=1000)\n","\n","        # Merge corrected rows back into the original DataFrame\n","        corrected_rows_df = pd.DataFrame(corrected_rows, columns=['marke', 'modelis', 'cleaned_marke', 'cleaned_modelis'])\n","        df_gpt_cleaned = df_gpt_cleaned.merge(\n","            corrected_rows_df[['marke', 'modelis', 'cleaned_marke', 'cleaned_modelis']],\n","            on=['marke', 'modelis'],\n","            how='left',\n","            suffixes=('', '_corrected')\n","        )\n","\n","        # Update columns with corrected values\n","        df_gpt_cleaned['cleaned_marke'] = df_gpt_cleaned['cleaned_marke_corrected'].combine_first(df_gpt_cleaned['cleaned_marke'])\n","        df_gpt_cleaned['cleaned_modelis'] = df_gpt_cleaned['cleaned_modelis_corrected'].combine_first(df_gpt_cleaned['cleaned_modelis'])\n","        df_gpt_cleaned.drop(columns=['cleaned_marke_corrected', 'cleaned_modelis_corrected'], inplace=True)\n","\n","    # Save the final cleaned dataset\n","    print(f\"Saving final cleaned dataset to {output_path}...\")\n","    save_dataset(df_gpt_cleaned, output_path)\n","    print(\"Pipeline completed.\")\n","\n","main_path = '/content/drive/MyDrive/Master Paper/Data/'\n","\n","# Run pipeline for Lithuanian dataset\n","chatgpt_cleaning_pipeline(\n","    file_path=f\"{main_path}/unique_marke_modelis.xlsx\",\n","    output_path=f\"{main_path}/unique_marke_modelis_cleaned.csv\",\n","    dataset_type='lithuanian',\n","    drop_columns=['nr', 'Unnamed: 3'],\n","    gpt_output_file=f\"{main_path}lithuanian_gpt_cleaned_temp.csv\"\n",")\n","\n","# Run pipeline for European dataset\n","chatgpt_cleaning_pipeline(\n","    file_path=f\"{main_path}/ea_europa_real_world_data_cars_vans/cleaned_2022_Vans_Raw.csv\",\n","    output_path=f\"{main_path}/ea_europa_cleaned.csv\",\n","    dataset_type='european',\n","    gpt_output_file=f\"{main_path}european_gpt_cleaned_temp.csv\"\n",")"],"metadata":{"id":"6rB1gvH-oHMY"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1CvuL8SGVlsM215UvEOPkMALSm8-M0xCD","authorship_tag":"ABX9TyPZRFFlYv9+0Shf7UMn/xUl"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}